{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will train models to predict whether a complaint is closed or closed with some sort of relief. The features used to predict this include details about the complaint (i.e., the product) and features derived from the narrative of the complaint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (learning_curve, StratifiedShuffleSplit, cross_val_score, ShuffleSplit,\n",
    "                                     cross_val_predict, RandomizedSearchCV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, need to get the data that is housed in PostgreSQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set-up access to database\n",
    "db_name = 'complaint1'\n",
    "username = 'postgres'\n",
    "host = 'localhost'\n",
    "port = '5432' \n",
    "#password = 'pw'\n",
    "\n",
    "con = psycopg2.connect(database=db_name, \n",
    "    host='localhost',\n",
    "    user=username,\n",
    "    password=password)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM complaint1;\n",
    "\"\"\"\n",
    "complaints_df = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the narratives might have missing values after pre-processing, so we'll remove any that are empty now\n",
    "complaints_df=complaints_df.dropna(subset = ['narrative'])\n",
    "complaints_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good. We had one complaint that was empty after the text pre-processing, so it was removed from consideration. We will now move forward with data preparation.\n",
    "\n",
    "We now need to get only some features out of the complaints_df that are relevant to our prediction task. We'll start by obtaining information about the complaint, the narrative submited by the consumer and meta-features about the complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feat=['sentiment','ADJ','ADP','ADV','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON',\n",
    "          'PROPN','PUNCT','SPACE','SYM','VERB','X','avg_words_sent','num_sent','num_word']\n",
    "\n",
    "#select these features from the full data set\n",
    "X = complaints_df[meta_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one last time, any data that is missing, get rid of it\n",
    "X_cleaned = X[~X.isnull().all(axis=1)]\n",
    "\n",
    "#complete fill-in so that scikit learn will work\n",
    "X_cleaned = X_cleaned.fillna(0)\n",
    "\n",
    "#standardize feature for models later\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_cleaned)\n",
    "\n",
    "#save the scaler so it can be used in the Flask App later\n",
    "joblib.dump(scaler, 'trained_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A future consideration is to also include state and product information in the predictive models. We'll save this information now so we can come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get other descriptors about the complaints\n",
    "desc_feat=['prod','state']\n",
    "\n",
    "X2 = complaints_df[desc_feat]\n",
    "\n",
    "#delete rows with missing data\n",
    "X2_cleaned = X2[~X2.isnull().all(axis=1)]\n",
    "\n",
    "#fill remaining missing values with zero\n",
    "X2_cleaned = X2_cleaned.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform one-hot encoding (or dummy coding) for these features\n",
    "#first, perform one-hot encoding for the column prod\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "prod_vals=array(X2_cleaned['prod'])\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_prod = label_encoder.fit_transform(prod_vals)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=True)\n",
    "integer_encoded_prod = integer_encoded_prod.reshape(len(integer_encoded_prod), 1)\n",
    "onehot_encoded_prod = onehot_encoder.fit_transform(integer_encoded_prod)\n",
    "\n",
    "onehot_encoded_prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do the same for states\n",
    "X2_cleaned['state'] = X2_cleaned['state'].astype(str)\n",
    "state_vals=array(X2_cleaned['state'])\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_state = label_encoder.fit_transform(state_vals)\n",
    "\n",
    "integer_encoded_state = integer_encoded_state.reshape(len(integer_encoded_state), 1)\n",
    "onehot_encoded_state = onehot_encoder.fit_transform(integer_encoded_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate the bag-of-words matrix that will be used in concert with the meta-features to train the classifiers. We'll use unigrams and bigrams with the tf-idf transformation to sort out important, rare terms from those that are not predictive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, get the narratives out so that we can generate a bag-of-words representation for these texts\n",
    "#narratives=complaints_df['narrative']\n",
    "\n",
    "#generate matrix\n",
    "#vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=250)\n",
    "#X_ngrams = vectorizer.fit_transform(narratives)\n",
    "#joblib.dump(X_ngrams, 'x_ngrams.pkl')\n",
    "#save vectorizer so it can be used in later cases\n",
    "#joblib.dump(vectorizer, 'tfidf_unibi_250.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save matrix to save time later\n",
    "#sparse.save_npz(\"ngrams.npz\", X_ngrams)\n",
    "X_ngrams = sparse.load_npz(\"ngrams.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, need to merge everything together: the meta-features, the descriptions of the complaint, and the n-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std_sparse = sparse.csr_matrix(X_std)\n",
    "#X2_sparse0= sparse.csr_matrix(X2_cleaned)\n",
    "#X2_sparse0=sparse.csr_matrix(X2_cleaned)\n",
    "X_full = sparse.hstack([X_std_sparse, onehot_encoded_prod, onehot_encoded_state, X_ngrams])\n",
    "X_nongrams = sparse.hstack([X_std_sparse, onehot_encoded_prod, onehot_encoded_state])\n",
    "X_full2 = sparse.hstack([X_std_sparse, X_ngrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last step, after getting the feature matrix merged, is to encode the response variable to be used in later models. We'll do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=complaints_df['response']\n",
    "#y['response'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first explore models using only the meta-features: descriptions of the complaints, the product, and the state of the person filing the complaint. We'll use stochastic gradient descent because the data are quite sparse (especially when we use the meta-features in concert with the n-grams). \n",
    "\n",
    "To gauge performance of the classifiers, we will use 10-fold cross validation and focus on precision (i.e., limiting the proportion of false positives identified by the classifier). Also, the data are quite imblanaced. Most people do not receive relief when filing their complaints, therefore, we'll also perform stratified sampling to divide the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommended number of iterations for SGD from scikit learn documentation\n",
    "SGD_iterations = np.ceil(10 ** 6 / len(X_std))\n",
    "SGD_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SGD with log loss (i.e., logistic regression) and the elastic net penalty (in case some predictors are correlated)\n",
    "#set the max iterations equal to the recommended iterations from scikit learn documentation, set random state to 129\n",
    "#use stratified shuffle split to stratify the data based on the response variable due to imbalance\n",
    "#score based on precision and use all processing cores \n",
    "scores = cross_val_score(estimator=SGDClassifier(loss='log', penalty='elasticnet', max_iter=SGD_iterations, \n",
    "                                                 random_state=28),\n",
    "    X=X_nongrams,y=y_enc,cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=28),\n",
    "    scoring='precision',n_jobs=1)\n",
    "\n",
    "#average and standard deviation of precision across folds\n",
    "print('Precision: {} +/- {}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the confusion matrix to see what sort of errors the classifier is making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform prediction to get the confusion matrix \n",
    "y_pred = cross_val_predict(estimator=SGDClassifier(loss='log', penalty='elasticnet', max_iter=SGD_iterations,\n",
    "        random_state=28),\n",
    "    X=X_nongrams, y=y_enc, cv=10, n_jobs=-1)\n",
    "\n",
    "#get confusion matrix\n",
    "cm = metrics.confusion_matrix(y_enc, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, looks like the classifier performance is pretty weak with respect to precision. There are very few true positives, and a lot of false negatives (not surprising given the significant imbalance). On the other hand, there is a non-trivial amount of false positives (1627), so we will explore if the n-grams improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing as before, just with more features\n",
    "scores_full = cross_val_score(estimator=SGDClassifier(loss='log', penalty='elasticnet', max_iter=SGD_iterations, \n",
    "                                                 random_state=28),\n",
    "    X=X_full2,y=y_enc,cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',n_jobs=1)\n",
    "\n",
    "#average and standard deviation of precision across folds\n",
    "print('Precision: {} +/- {}'.format(scores_full.mean(), scores_full.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's have a look at the confusion matrix to see where the classifier is making errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform prediction to get the confusion matrix \n",
    "y_pred = cross_val_predict(estimator=SGDClassifier(loss='log', penalty='elasticnet', max_iter=SGD_iterations,\n",
    "        random_state=41),\n",
    "    X=X_full2, y=y_enc, cv=10, n_jobs=1)\n",
    "\n",
    "#examine confusion matrix\n",
    "cm = metrics.confusion_matrix(y_enc, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix here shows better performance than when we only used the meta-features. We see here that 2060 are correctly identified as receiving relief, but 61086 are incorrectly identified as false negatives. SVMs have been shown to work well for text classification, so let's try to see how an SVM with linear kernel performs. Due to the rather significant increase in performance when using the n-grams in the elastic net model, we'll move forward with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same thing as before, just with more features\n",
    "scores_full_svm = cross_val_score(estimator=SGDClassifier(loss='hinge', penalty='l2', max_iter=SGD_iterations, \n",
    "                                                 random_state=41),\n",
    "    X=X_full2,y=y_enc,cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision',n_jobs=1)\n",
    "\n",
    "#average and standard deviation of precision across folds\n",
    "print('Precision: {} +/- {}'.format(scores_full_svm.mean(), scores_full_svm.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are pretty bad, much worse than when we used regularized regression. With more time, we could consider other classifiers, such as neural networks, but for now, we'll focus on using the lasso logistic regression model because (a) it is easy to understand, (b) results could improve when we train the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to train the hyperparameters to determine whether this improves the predictive power of the model. We'll use nested cross-validation to learn the optimized hyperparameters. Because we are using the elastic net, we need to determine the mixing parameter (l1_ratio) and the optimal penalty parameter (alpha, but traditionally referred to as lambda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to initialize the search space for the parameters alpha and l1ratio\n",
    "param_distn = {'alpha': np.logspace(-6, -1, 10),'l1_ratio': np.linspace(0.05, 0.95, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up randomized search of hyperparameters for efficiency; we'll use stratified sampling as before due to imbalance\n",
    "grid_search = RandomizedSearchCV(estimator=SGDClassifier(loss='log', penalty='elasticnet', max_iter=SGD_iterations,\n",
    "        random_state=41),\n",
    "   param_distributions=param_distn, cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41),\n",
    "    scoring='precision', n_jobs=1)\n",
    "\n",
    "#peform the search\n",
    "scores = cross_val_score(estimator=grid_search, X=X_full2, y=y_enc,\n",
    "                         cv=StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=41), scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the scores to see if performance improved\n",
    "#scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Final Model and Saving It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use logistic regression with the elastic net to classify complaints as receiving relief or not receiving relief. To do so, we'll fit the models using the entire data set and the optimal hyperparameters computed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the randomized hyperparameter search to identify optimal \n",
    "# hyperparameters\n",
    "grid_search.fit(X_full2, y_enc)\n",
    "\n",
    "# Train the classifier on the entire dataset using optimal hyperparameters\n",
    "clf_full = SGDClassifier(\n",
    "        loss='log',\n",
    "        penalty='elasticnet',\n",
    "        alpha=grid_search.best_params_['alpha'],\n",
    "        l1_ratio=grid_search.best_params_['l1_ratio'],\n",
    "        max_iter=SGD_iterations,\n",
    "        random_state=41\n",
    ")\n",
    "clf_full.fit(X_full2, y_enc);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it in a pickle so that we can use it in the Flask App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf_full, 'trained_classifier.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
