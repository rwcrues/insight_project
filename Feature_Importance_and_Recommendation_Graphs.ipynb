{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Final Model and Determining Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the final model has been trained with optimal hyperparameters, we want to understand which features are important so that we can provide actionable insights to users of the tool. We first fit the final model. To do this, we'll fit the final model, examine which unigrams/bigrams and meta-features are indicative of receiving relief.\n",
    "\n",
    "The first step we need to do is to call the data from the PostgreSQL, standardize some of the features, call the n-grams constructed previously, and fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (learning_curve, StratifiedShuffleSplit, cross_val_score, ShuffleSplit,\n",
    "                                     cross_val_predict, GridSearchCV)\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(context='notebook', style='darkgrid')\n",
    "sns.set(font_scale=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Postgres credentials/read in complaint database\n",
    "db_name = 'complaint1'\n",
    "username = 'postgres'\n",
    "host = 'localhost'\n",
    "port = '5432' \n",
    "#password = ''\n",
    "\n",
    "con = psycopg2.connect(database=db_name, \n",
    "    host='localhost',\n",
    "    user=username,\n",
    "    password=password)\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM complaint1;\n",
    "\"\"\"\n",
    "complaints_df = pd.read_sql_query(sql_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the narratives might have missing values after pre-processing, so we'll remove any that are empty now\n",
    "complaints_df=complaints_df.dropna(subset = ['narrative'])\n",
    "complaints_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feat=['sentiment','ADJ','ADP','ADV','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON',\n",
    "          'PROPN','PUNCT','SPACE','SYM','VERB','X','avg_words_sent','num_sent','num_word']\n",
    "\n",
    "#select these features from the full data set\n",
    "X = complaints_df[meta_feat]\n",
    "\n",
    "# Remove all rows with no data\n",
    "X_cleaned = X[~X.isnull().all(axis=1)]\n",
    "\n",
    "# Fill remaining missing values with zero\n",
    "X_cleaned = X_cleaned.fillna(0)\n",
    "\n",
    "# Standardize the meta features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X_cleaned)\n",
    "\n",
    "X_ngrams = sparse.load_npz(\"ngrams.npz\")\n",
    "\n",
    "X_std_sparse = sparse.csr_matrix(X_std)\n",
    "X_full = sparse.hstack([X_std_sparse, X_ngrams])\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to load the classifier trained on the full data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_class = joblib.load('trained_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the Most Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will consider is how the whole data set (meta features and n-grams) are predictive of receiving relief. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the pickle of the vectorizer for the n-grams\n",
    "vectorizer = joblib.load('tfidf_unibi_250.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine meta feature labels with n-gram labels\n",
    "all_features = meta_feat + vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the corresponding feature names to the parameters, sorted from highest\n",
    "# to lowest\n",
    "feature_ranks = pd.Series(\n",
    "    trained_class.coef_.T.ravel(),\n",
    "    index=all_features\n",
    ").sort_values(ascending=False)[:19][::-1]\n",
    "\n",
    "# Display a bar graph of the top features\n",
    "graph = feature_ranks.plot(\n",
    "    kind='barh',\n",
    "    legend=False,\n",
    "    figsize=(4, 8),\n",
    "    color='#666666'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some of the key words are related to credit bureaus, feeds people are charged, and fraudulent charges. Let's now take a look at the meta-features to figure out which of those are predictive of receiving relief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the corresponding meta feature names to the parameters, sorted from\n",
    "# highest to lowest\n",
    "meta_feature_ranks = pd.Series(\n",
    "    trained_class.coef_.T.ravel()[:len(meta_feat)],\n",
    "    index=meta_feat\n",
    ").sort_values(ascending=False)[::-1]\n",
    "\n",
    "# Display a bar plot of the meta feature importance\n",
    "graph2 = meta_feature_ranks.plot(\n",
    "    kind='barh',\n",
    "    legend=False,\n",
    "    figsize=(5, 8),\n",
    "    color='#666666'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like having a complaint that is positive and clearly written is more likely to be successful in obtaining relief. The use of adjectives indicates that the complaint should be descriptive, and punctuation likely indicates that those writing in should be concise (number of sentences is negatively related with receiving relief). Determiners are likely needed to make the writing clearer. \n",
    "\n",
    "Alternatively, making a very long complaint is probably not a good idea. Likewise, we see including coordinating conjunctions and number of sentences as being problematic. Likewise, including interjections and participles (i.e., 's to show ownership) are related to not receiving relief.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the predictive features of receiving relief \n",
    "pred_feat=['sentiment','SYM','DET','PUNCT','ADJ','VERB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Recommendations/Actionable Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that users can modify their narrative complaints to make them more likely to receive relief, we will compare the users submitted narrative with those that were successful. To do so, we first need to identify the narratives in our training set that were successful. Then, we will need to pre-process this data so that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "complaints_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset only successful complaints\n",
    "success=complaints_df[complaints_df.response=='relief']\n",
    "success.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just subset the meta-features to provide recommendations\n",
    "success_meta=success[meta_feat]\n",
    "#get rid of rows that don't contain any information\n",
    "success_cleaned = success_meta[~success_meta.isnull().all(axis=1)]\n",
    "#fill in missing values\n",
    "success_cleaned = success_cleaned.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the average of each of the meta features for successful complaints \n",
    "avg_meta_success = success_cleaned.mean()\n",
    "\n",
    "#standardize the meta features\n",
    "success_meta_std = pd.Series(scaler.transform([avg_meta_success]).ravel(), index=meta_feat)\n",
    "\n",
    "#save these results so they can be easily called on for webapp\n",
    "joblib.dump(success_meta_std, 'success_meta_vector.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Recommendations From a Narrative Complaint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to compare the features in a complaint to complaints that were successful. To do that, we'll use a tester complaint that I made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feat_eng\n",
    "tester=(\"I am really disappointed by Wells Fargo. I opened up a savings account with them a few years ago because they had a good rate; however, I suddenly realized they had opened many new accounts under my name. I found out about this problem when I pulled my credit report. I'm not sure who, but someone had opened dozens of accounts at the bank in my name. There were several checking accounts and savings accounts that were charged tons of fees, totalling around $300. When I called, I was told that these accounts were all mistakes and my money would be refunded, but unfortunately, it has not been refunded. I'm writing to get the money that is owed to me refunded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the meta features for the tester complaint, to do so, we will call on the commands developed in the feat_eng \n",
    "#file\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(text):\n",
    "    score = analyser.polarity_scores(text)\n",
    "    lb = score['compound']\n",
    "    if lb >= 0.05:\n",
    "        return 1\n",
    "    elif (lb > -0.05) and (lb < 0.05):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp=spacy.load('en')\n",
    "\n",
    "def postag(text):\n",
    "    doc=nlp(text)\n",
    "    pos=[(i, i.pos_) for i in doc]\n",
    "    counts=Counter(tag for word, tag in pos)\n",
    "    return counts\n",
    "\n",
    "def sent_word_tok(text):\n",
    "    sents=nltk.sent_tokenize(text)\n",
    "    words=nltk.word_tokenize(text)\n",
    "    num_sents=len(sents)\n",
    "    num_words=len(words)\n",
    "    \n",
    "    if num_words == 0:\n",
    "        avg_word_sent == 0\n",
    "    else:\n",
    "        avg_word_sent = num_words/num_sents\n",
    "    return {'num_word': num_words, 'num_sent': num_sents, 'avg_words_sent': avg_word_sent}\n",
    "\n",
    "def meta_calc(narrative):\n",
    "    \n",
    "    #take the narrative text and analyze it so that we can get a prediction\n",
    "    # first, get sentiment\n",
    "    senti = sentiment_analyzer_scores(narrative)\n",
    "    # Take input text and get POS tags\n",
    "    pos = postag(narrative)\n",
    "    # Take input text and get summary statistics about length\n",
    "    length = sent_word_tok(narrative)\n",
    "\n",
    "    #create pos table so all variables are present\n",
    "    pos_df=pd.DataFrame()\n",
    "    pos_df[\"ADJ\"]=0; pos_df[\"ADP\"]=0; pos_df[\"ADV\"]=0; pos_df[\"CCONJ\"]=0; \n",
    "    pos_df[\"DET\"]=0; pos_df[\"INTJ\"]=0; pos_df[\"NOUN\"]=0; pos_df[\"NUM\"]=0; \n",
    "    pos_df[\"PART\"]=0; pos_df[\"PRON\"]=0; pos_df[\"PROPN\"]=0; pos_df[\"PUNCT\"]=0; \n",
    "    pos_df[\"SPACE\"]=0; pos_df[\"SYM\"]=0; pos_df[\"VERB\"]=0; pos_df[\"X\"]=0\n",
    "\n",
    "    #change these all to a pandas data frame and concatenate them\n",
    "    senti_df=pd.DataFrame(pd.Series(senti))\n",
    "    senti_df.columns=['senti']\n",
    "    pos_df_data=pd.DataFrame(pos,index=[0])\n",
    "    pos_fin=pos_df.append(pos_df_data)\n",
    "    pos_fin=pos_fin.fillna(0)\n",
    "    length_df=pd.DataFrame(length,index=[0])\n",
    "    #concatenate these to form meta-feature vector\n",
    "    meta_feat=pd.merge(senti_df,pos_fin,left_index=True, right_index=True)\n",
    "    meta_feat=pd.merge(meta_feat,length_df,left_index=True, right_index=True)\n",
    " \n",
    "    #generate the output\n",
    "    return meta_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feat_test=meta_calc(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardize the feature vector\n",
    "scaler=joblib.load('trained_scaler.pkl')\n",
    "feature_vector_std = pd.Series(scaler.transform(meta_feat_test).ravel(),index=meta_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute meta feature ranks\n",
    "feature_ranks = pd.Series(trained_class.coef_.T.ravel()[:len(meta_feat)], index=meta_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weighted score of the meta features of a narrative\n",
    "user_nar_score = np.multiply(feature_vector_std[pred_feat],feature_ranks[pred_feat])\n",
    "user_nar_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weighted score of the meta features of successful narratives\n",
    "suc_nar_score = np.multiply(success_meta_std[pred_feat],feature_ranks[pred_feat])\n",
    "suc_nar_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the weighted scores into a single DataFrame\n",
    "messy = pd.DataFrame([user_nar_score, suc_nar_score], index=['Your Narrative', 'Successful Narratives']).T.reset_index()\n",
    "\n",
    "# Transform the combined data into tidy format\n",
    "tidy = pd.melt(messy,id_vars='index', value_vars=['Your Narrative', 'Successful Narratives'],var_name=' ')\n",
    "messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw a grouped bar plot of the weighted scores\n",
    "fig = sns.factorplot(\n",
    "    data=tidy,\n",
    "    y='index',\n",
    "    x='value',\n",
    "    hue=' ',\n",
    "    kind='bar',\n",
    "    size=5,\n",
    "    aspect=1.5,\n",
    "    palette='Set1',\n",
    "    legend_out=False\n",
    ").set(\n",
    "    xlabel='score',\n",
    "    ylabel='',\n",
    "    xticks=[]\n",
    ")\n",
    "# Re-label the y-axis and reposition the legend\n",
    "['sentiment','SYM','DET','PUNCT','ADJ','VERB']\n",
    "labels = ['Sentiment','Symbols','Determiners','Punctuation','Adjectives','Verbs']\n",
    "plt.yticks(np.arange(len(pred_feat)), labels)\n",
    "fig.ax.legend(loc='lower right');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
